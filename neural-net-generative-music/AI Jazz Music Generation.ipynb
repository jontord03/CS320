{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jonathon Tordilla\n",
    "\n",
    "4 December 2019\n",
    "\n",
    "# AI Jazz Generation\n",
    "\n",
    "# Important: Citations are at the end of this file\n",
    "\n",
    "For my final project, I've decided to look to my favorite programming youtuber, \"carykh,\" for inspiration. He's worked with machine learning quite a bit in the past, and he's posted several highly entertaining animated videos about the subject. Since I first discovered his channel about a year ago, I've always wanted to try machine learning for myself. His channel is one of the reasons why I decided to enroll in this course: Introduction to Machine Learning. The very first video I discovered from his channel was titled \"AI evolves to compose 3 hours of jazz!\" Because I'm a music fan (10 years of piano lessons and basic instruction in guitar and drums), I think I might be able to offer even more insight about the music my model will eventually evolve to create. I'll add an additional level of analysis to my results due to my musical background, and I'll enjoy applying my knowledge of neural networks to this project. \n",
    "\n",
    "Cary begins his video by saying \"I decided I really wanted to get my computer to generate jazz on its own\" (Huang 1-6). This was not his first time generating music with machine learning, because his original plan was to discover whether or not neural networks could be used to generate pleasant baroque music. However, the end result sounded eerily like jazz, so Cary was curious as to what the model would generate if the training data was specifically composed in that style. The new training set consisted of piano pieces composed by Doug McKenzie, and the model attempted to replicate the \"style\" of the music as close as possible without copying it directly. After several hours of training, promising samples were generated by the \"PixelCNN\" model Cary used (created by Aaron van den Oord and several other people). Although many of the generated pieces (there were 700 in total) aren't worth listening to, some of the pieces are quite elaborate, elegant, and surprisingly creative. Overall, Cary's attempt to generate jazz music was a success. \n",
    "\n",
    "<img src=\"cary.png\"/>\n",
    "\n",
    "For my interpretation of this project, I would like to use a different training dataset. Cary trained his model on Doug McKenzie compositions, but I'm not a huge fan of his work (I just don't listen to his pieces very often). I would like to train the model on compositions by an artist I'm familiar with: Vince Guaraldi. As for the data processing techniques Cary used to convert the music into a form useful for a neural network, I'll simply consult his video \"AI evolves to generate 3 hours of jazz music!\" for guidance. I'm quite interested in generative neural networks, and this project will be a thrilling introduction to the subject. \n",
    "\n",
    "<img src=\"vince.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effectively implement the techniques demonstrated in Cary's video, I'll need to watch it carefully and gather the appropriate resources for this project. There's a possibility that my computer does not have the computational capabilities to generate music of similar quality, but I'll try to conduct this experiment nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main technique behind using \"HyperGAN\" to generate music is to represent notes as images. HyperGAN is built to generate images, using a deconvolutional neural network. This works because, (this is oversimplified) the model starts with a random low-resolution image and applies upscalings and various visual alterations to generate the output. HyperGAN \"looks at thousands of training images from the outside world, and then generates new images in the same original style\" (Huang 138-144). More specifically, according to an article by Mark Farragher, deconvolutional neural networks operate somewhat similarly to convolutional neural networks, but almost in reverse. To begin to understand how they work, recall that convolutional neural networks are helpful for classifying what an image is. An example might be something along the lines of: a user wants to know the type of plant he's picking because it's critical to know whether or not it's poisonous. So he takes a picture of the plant and after a bunch of flashing lights and concerning sounds, his computer tells him that the plant is \"Rosmarinus officianlis\" which can cause miscarriage. The input in this case was a picture, and the generated result was a single word: \"rosemary.\" The question behind the creation of the DCNN was \"can we run a CNN in reverse?\" (Farragher). The fundamental architecture of a deconvolutional neural network looks something like the image below (also taken from the Farragher article): \n",
    "<img src=\"dcnn.png\"/>\n",
    "If you read this image from the right to the left, it looks almost identical to your typical convolutional neural network. It takes an input image, examines specific groups of pixels with different neurons(giving the \"locally-connected layer.\" See https://jennselby.github.io/MachineLearningCourseNotes/#convolutional-neural-networks for more information), shares parameters between neurons, and activates these neurons whenever a pattern appears in the image. Using this information, the model is able to effectively train on an image dataset and classify the image as a certain class vector. However, the deconvolutional neural network works in the opposite direction, and the main purpose of using such a network is to generate somewhat original content, such as a picture of a chair (depicted in the image) or possibly 3 hours jazz music. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effectively perform a similar experiment as Cary, I'll need to follow his procedure, changing only the training dataset. Cary used MIDI files as his training dataset, and he plotted the notes on a two-dimensional graph. This created an image that could easily be interpreted by HyperGAN. Y-values represent pitch in semitone intervals, and x-values represent time in 20th of a second intervals (depicted below). \n",
    "<img src=\"notes.png\"/>\n",
    "The music images were scaled to 64x96 pixels, so the training data wasn't too large. However, my computer is significantly slower than Cary's, so I won't be entirely sure until I train my model. Cary also made another modification to the images in order to represent more information per image. Each color channel on the image was made to represent a single pixel, and certain channels were turned on or off to represent different colors and ultimately when certain notes were to be played. This allows the amount of information to be tripled, as 27 bits of information can be represented by only nine pixels (depicted below). \n",
    "<img src=\"pixel.png\"/>\n",
    "There are various \"pros and cons\" to this color channel representation approach, but I won't list them here. Instead, here's a link to Cary's video which starts immediately when these advantages and disadvantages are explained: [Cary's video](https://youtu.be/nA3YOFUCn4U?t=238). Although the end result wasn't very pleasurable to the human ear (and it took about 4 days to train), I'm not exactly looking for creating elegant music. My expectations for my results are extremely low, and I'm hoping to generate an output the at least somewhat resembles music to some extent. I've decided to use HyperGAN to generate jazz music. If I have time, I'll attempt to use PixelCNN because its generated results were far more musical than HyperGAN. However, I'm not especially picky about the quality of the music produced by my model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that I've only just realized is that Cary does not explain how exactly he converted the MIDI files to a comprehensible image. However, he explains this process in a previous video titled \"[Computer evolves to generate baroque music!](https://www.youtube.com/watch?v=SacogDL_4JU&t=88s)\" Cary first visited a website with several free midi files of bach's music. He then used a midi to csv program to create a single text file storing the collection of compositions. After all of this data gathering and processing, Cary noticed something interesting. In the midi/csv file, there is a bit of text in every line which says \"Note_on\" or \"Note_off\" (depicted below). \n",
    "<img src=\"note.png\"/>\n",
    "Cary changed this using a processing script and represented each key of a piano with a different character on the computer keyboard (depicted below). \n",
    "<img src=\"processing.png\"/>\n",
    "Additionally, to allow for different keys to play at the same time, Cary created spaces between notes which represented the passage of time. \n",
    "<img src=\"time.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you might be wondering, \"I thought we were going to use HyperGAN to train our dataset? Doesn't that require us to render our data in image form?\" \n",
    "That's fair. As of now, I don't think I'll be using HyperGAN to train my data. Instead, Cary uses an LSTM for his baroque music video, so that's the model I'll use as well. \n",
    "To understand how an LSTM works, think about it as a recurrent neural network with a \"long-term memory unit.\" This is used as a means of storage for long-term information. The great thing about LSTMs is that it can remember information without having it affect the outputs of neurons directed towards other layers (information taken from Jen Selby's course notes). According to Jen Selby, each layer of the LSTM is made up of four recurrent layers which represent the long-term memory unit. They also contain three gates which perform the following functions: forget (some long-term data is deleted), input (weights decide what information will be added to the long-term memory unit), and the output (weights decide what will be routed to the output). \n",
    "Although I went through all the trouble of describing how it operates, the neural network I'll be using for this project will be an LSTM, which is useful because it takes text as its input. \n",
    "\n",
    "After training the data, the LSTM outputs its generated midi file in its compressed form. A processing script is used to convert it into the csv/midi form, and a csv-midi converter is used to create the final audible music file.\n",
    "\n",
    "Although I'm interested in compression, I don't think I'll write as long of a script to compress the midi/csv files as Cary did. I'm mostly just interested in converting a midi file to csv and feeding that directly into a LSTM. Although the file size is approximately 50 times greater, I'll probably devise a method to make the file size smaller using python (it'll be in this jupyter notebook). \n",
    "\n",
    "Below will be all of the data processing and LSTM setup and training, and I'll explain what I'm doing along the way with comments. After I generate some results, I'll write a reflection about the experience and how I can improve my methods for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Plan: \n",
    "\n",
    "#1. Midi to CSV\n",
    "#2. CSV \"compression\"\n",
    "#3. LSTM setup\n",
    "#4. LSTM training\n",
    "#5. CSV \"decompression\"\n",
    "#6. CSV to Midi\n",
    "#7. Play with Garageband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I've downloaded approximately 30-40 minutes of music, so \n",
    "#I'll convert all of the files to csv and paste them all \n",
    "#into a single document I can train the LSTM with. To convert\n",
    "#the files to csv, I'll be using py_midicsv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_midicsv as pm\n",
    "#midi to csv\n",
    "cast_your_fate = pm.midi_to_csv(\"midi/cast_your_fate.mid\")\n",
    "linus_and_lucy = pm.midi_to_csv(\"midi/linus_lucy.mid\")\n",
    "medley = pm.midi_to_csv(\"midi/medley_ok.mid\")\n",
    "my_drum = pm.midi_to_csv(\"midi/my_drum.mid\")\n",
    "o_tannenbaum = pm.midi_to_csv(\"midi/o_tannenbaum_dwb.mid\")\n",
    "christmas_coming = pm.midi_to_csv(\"midi/christmas_coming.mid\")\n",
    "christmastime = pm.midi_to_csv(\"midi/christmastime.mid\")\n",
    "skating = pm.midi_to_csv(\"midi/skating.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in dataframe form\n",
    "import pandas as pd\n",
    "#to dataframe\n",
    "df_cast = pd.DataFrame(cast_your_fate)\n",
    "df_linus = pd.DataFrame(linus_and_lucy)\n",
    "df_medley = pd.DataFrame(medley)\n",
    "df_drum = pd.DataFrame(my_drum)\n",
    "df_tannenbaum = pd.DataFrame(o_tannenbaum)\n",
    "df_christmas_coming = pd.DataFrame(christmas_coming)\n",
    "df_christmastime = pd.DataFrame(christmastime)\n",
    "df_skating = pd.DataFrame(skating)\n",
    "\n",
    "#saving as csv file\n",
    "#csv_cast_fate = df_cast.to_csv('cast_your_fate.csv')\n",
    "#csv_linus = df_linus.to_csv('linus_lucy.csv')\n",
    "#csv_medley = df_medley.to_csv('medley.csv')\n",
    "#csv_drum = df_drum.to_csv('my_drum.csv')\n",
    "#csv_tannenbaum = df_tannenbaum.to_csv('tannenbaum.csv')\n",
    "#csv_christmas_coming = df_christmas_coming.to_csv('christmas_coming.csv')\n",
    "#csv_christmastime = df_christmastime.to_csv('christmastime.csv')\n",
    "#csv_skating = df_skating.to_csv('cast_your_fate.csv')\n",
    "\n",
    "#concat df \n",
    "df_jazz = pd.concat([df_cast, df_linus, df_medley, df_drum, \n",
    "        df_tannenbaum, df_christmas_coming, df_christmastime, \n",
    "        df_skating], axis=1, join='inner')\n",
    "#saving as a csv file\n",
    "csv_jazz = df_jazz.to_csv('jazz.csv') #724 kB of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a basic LSTM to generate text using the following link: \n",
    "\n",
    "https://towardsdatascience.com/automatically-generate-hotel-descriptions-with-lstm-afa37002d4fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0, 0, Header, 1, 6, 192\\n</td>\n",
       "      <td>0, 0, Header, 1, 11, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 19, 120\\n</td>\n",
       "      <td>0, 0, Header, 1, 11, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 15, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 10, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 12, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 10, 96\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1, 0, Text_t, \"Cast Your Fate To The Wind\"\\n</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Key_signature, -3, \"major\"\\n</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Sequencer_specific, 03, 00, 00, 41\\n</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0  \\\n",
       "0                     0, 0, Header, 1, 6, 192\\n   \n",
       "1                           1, 0, Start_track\\n   \n",
       "2  1, 0, Text_t, \"Cast Your Fate To The Wind\"\\n   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 11, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                    0  \\\n",
       "0          0, 0, Header, 1, 19, 120\\n   \n",
       "1                 1, 0, Start_track\\n   \n",
       "2  1, 0, Key_signature, -3, \"major\"\\n   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 11, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 15, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 10, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                            0  \\\n",
       "0                   0, 0, Header, 1, 12, 96\\n   \n",
       "1                         1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 03, 00, 00, 41\\n   \n",
       "\n",
       "                                                   0  \n",
       "0                          0, 0, Header, 1, 10, 96\\n  \n",
       "1                                1, 0, Start_track\\n  \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df_jazz.head(n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've decided to try a different approach: using the \"abc\" format of writing music instead of the csv/midi approach. The abc format is a lot more compressed and may be more suitable for my neural network purposes. However, this is just an exploration and I may try a different format in the future. The screenshot below is of the program I'll be using to obtain the jazz music/feed the generated text through. I'll also try playing the music on the piano myself eventually, and I'll probably post the link to a youtube video at the end of this \"tutorial.\" \n",
    "\n",
    "EasyABC Application\n",
    "<img src=\"abc.png\"/> \n",
    "More information on how to interpret the \"abc\" format can be found [here](http://www.lesession.co.uk/abc/abc_notation.htm#notes). \n",
    "\n",
    "However, because using the abc format is quite uncommon, it's a bit harder to find pre-formatted songs. So I've decided to peruse the \"abc tune finder\" [library](http://trillian.mit.edu/~jc/cgi/abc/find.cgi?P=charlie+brown&find=FIND&m=title&W=wide&scale=0.65&limit=1000&thresh=5&fmt=single&V=1&Tsel=tune&Nsel=0) in search for jazz music. I'll list the pieces I choose to train my model on below: \n",
    "\n",
    "1. Linus and Lucy\n",
    "2. What Is This Thing Called Love\n",
    "3. All Of Me\n",
    "4. All That Jazz - Clarinet\n",
    "5. All That Jazz - Lute\n",
    "6. After You've Gone\n",
    "7. Agua De Beber\n",
    "8. Bye Bye Blackbird\n",
    "9. Blue Bossa\n",
    "10. How Deep Is The Ocean\n",
    "11. Jazz Suite No. 2 Op. 99\n",
    "12. The Lady Is A Tramp\n",
    "\n",
    "Overall, the file size has been reduced by over 2000 percent, so I'm glad I chose the abc format. Now, I need to create a neural network and train my data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From a repo found [here](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py)\n",
    "\n",
    "The comments in the code are written by me or quoted from the repo. However, there may some comments that I accidently forgot to quote from the repo, but it's unlikely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 34933\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "#this just finds the file we need. I put all of the abc code into a text\n",
    "#file which I'll train the LSTM with. \n",
    "path = 'jazz_set.txt'\n",
    "with io.open(path, encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "#this prints the length of the dataset\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "#this will be used for vectorization, a process which allows a dataset to \n",
    "#be more computationally appropriate through vector operations, allowing\n",
    "#for training to be a much more efficient process, rather than a task which\n",
    "#may take up to hours or days at a time. For more information on vectorization, \n",
    "#visit this link: https://www.geeksforgeeks.org/vectorization-in-python/\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 11631\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# This cuts the text in sequences\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#Creating the LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # this serves to \"sample an index from a probability array\" and is\n",
    "    #used for each epoch\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        #This is the most interesting thing about this code. \n",
    "        #After each epoch, it returns generated text. This allows\n",
    "        #the user to observe the evolution of the model over time. \n",
    "        #If time permits, I'll make a recording of the progression \n",
    "        #of music evolution over time in addition to playing the \n",
    "        #pieces on a physical piano just for kicks. \n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "#Let's train!\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=60,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are saved in an 80 page long pdf titled \"generated_jazz.\"\n",
    "\n",
    "Something to note: in order to have correct syntax when writing abc code, you must always have this \"heading\" before any of the notes: \n",
    "\n",
    "X: 1\n",
    "\n",
    "T:Untitled\n",
    "\n",
    "C:Null\n",
    "\n",
    "Z:Null\n",
    "\n",
    "L:1/4\n",
    "\n",
    "Q:1/4=100\n",
    "\n",
    "K:C\n",
    "\n",
    "If this heading is missing, the EasyABC software will not recognize any of the generated music. Over time, the model may learn to include this in the generated music, but it's not guaranteed, and you may want to listen to what the music sounded like earlier on. So if you would like to generate music quickly before training the model for long periods of time, include the \"heading\" I included above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Now that I've generated quite a bit of jazz music, I'll see how it sounds when run through the abc IDE (EasyABC). I'll record the results and post the link below. \n",
    "\n",
    "# [Generated Music](https://youtu.be/v6BXvUeF1eY)\n",
    "\n",
    "If you've made it this far, thank you so much for reading this. I know I'm only a beginner in using machine learning for my own purposes, but it sure was a lot of fun working on this project. I'm hope you enjoyed reading it as I enjoyed writing it. Now you can try and generate your own music! It doesn't take any musical experience whatsoever. \n",
    "\n",
    "Happy Coding!\n",
    "\n",
    "-Jonathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>A list of resources is below:</i> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "Huang, Cary. \"AI evolves to compose 3 hours of jazz!\" <i>https://youtube.com,</i> https://www.youtube.com/watch?v=nA3YOFUCn4U. \n",
    "\n",
    "Huang, Cary. \"Computer evolves to generate baroque music!\" <i>https://youtube.com</i> https://youtube.com/watch?v=SacogDL_4JU&t=2s. \n",
    "\n",
    "https://medium.com/machinelearningadvantage/here-are-the-mind-blowing-things-a-deconvolutional-neural-network-can-do-2fc99e008fe4\n",
    "\n",
    "https://github.com/timwedde/py_midicsv\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://github.com/karpathy/char-rnn\n",
    "\n",
    "http://midkar.com/\n",
    "\n",
    "https://towardsdatascience.com/automatically-generate-hotel-descriptions-with-lstm-afa37002d4fc\n",
    "\n",
    "http://www.lesession.co.uk/abc/abc_notation.htm#notes\n",
    "\n",
    "http://trillian.mit.edu/~jc/cgi/abc/find.cgi?P=charlie+brown&find=FIND&m=title&W=wide&scale=0.65&limit=1000&thresh=5&fmt=single&V=1&Tsel=tune&Nsel=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
