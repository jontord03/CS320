{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jonathon Tordilla\n",
    "\n",
    "4 December 2019\n",
    "\n",
    "# AI Jazz Generation\n",
    "\n",
    "# Important: Citations are at the end of this file\n",
    "\n",
    "For my final project, I've decided to look to my favorite programming youtuber, \"carykh,\" for inspiration. He's worked with machine learning quite a bit in the past, and he's posted several highly entertaining animated videos about the subject. Since I first discovered his channel about a year ago, I've always wanted to try machine learning for myself. His channel is one of the reasons why I decided to enroll in this course: Introduction to Machine Learning. The very first video I discovered from his channel was titled \"AI evolves to compose 3 hours of jazz!\" Because I'm a music fan (10 years of piano lessons and basic instruction in guitar and drums), I think I might be able to offer even more insight about the music my model will eventually evolve to create. I'll add an additional level of analysis to my results due to my musical background, and I'll enjoy applying my knowledge of neural networks to this project. \n",
    "\n",
    "Cary begins his video by saying \"I decided I really wanted to get my computer to generate jazz on its own\" (Huang 1-6). This was not his first time generating music with machine learning, because his original plan was to discover whether or not neural networks could be used to generate pleasant baroque music. However, the end result sounded eerily like jazz, so Cary was curious as to what the model would generate if the training data was specifically composed in that style. The new training set consisted of piano pieces composed by Doug McKenzie, and the model attempted to replicate the \"style\" of the music as close as possible without copying it directly. After several hours of training, promising samples were generated by the \"PixelCNN\" model Cary used (created by Aaron van den Oord and several other people). Although many of the generated pieces (there were 700 in total) aren't worth listening to, some of the pieces are quite elaborate, elegant, and surprisingly creative. Overall, Cary's attempt to generate jazz music was a success. \n",
    "\n",
    "<img src=\"cary.png\"/>\n",
    "\n",
    "For my interpretation of this project, I would like to use a different training dataset. Cary trained his model on Doug McKenzie compositions, but I'm not a huge fan of his work (I just don't listen to his pieces very often). I would like to train the model on compositions by an artist I'm familiar with: Vince Guaraldi. As for the data processing techniques Cary used to convert the music into a form useful for a neural network, I'll simply consult his video \"AI evolves to generate 3 hours of jazz music!\" for guidance. I'm quite interested in generative neural networks, and this project will be a thrilling introduction to the subject. \n",
    "\n",
    "<img src=\"vince.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effectively implement the techniques demonstrated in Cary's video, I'll need to watch it carefully and gather the appropriate resources for this project. There's a possibility that my computer does not have the computational capabilities to generate music of similar quality, but I'll try to conduct this experiment nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main technique behind using \"HyperGAN\" to generate music is to represent notes as images. HyperGAN is built to generate images, using a deconvolutional neural network. This works because, (this is oversimplified) the model starts with a random low-resolution image and applies upscalings and various visual alterations to generate the output. HyperGAN \"looks at thousands of training images from the outside world, and then generates new images in the same original style\" (Huang 138-144). More specifically, according to an article by Mark Farragher, deconvolutional neural networks operate somewhat similarly to convolutional neural networks, but almost in reverse. To begin to understand how they work, recall that convolutional neural networks are helpful for classifying what an image is. An example might be something along the lines of: a user wants to know the type of plant he's picking because it's critical to know whether or not it's poisonous. So he takes a picture of the plant and after a bunch of flashing lights and concerning sounds, his computer tells him that the plant is \"Rosmarinus officianlis\" which can cause miscarriage. The input in this case was a picture, and the generated result was a single word: \"rosemary.\" The question behind the creation of the DCNN was \"can we run a CNN in reverse?\" (Farragher). The fundamental architecture of a deconvolutional neural network looks something like the image below (also taken from the Farragher article): \n",
    "<img src=\"dcnn.png\"/>\n",
    "If you read this image from the right to the left, it looks almost identical to your typical convolutional neural network. It takes an input image, examines specific groups of pixels with different neurons(giving the \"locally-connected layer.\" See https://jennselby.github.io/MachineLearningCourseNotes/#convolutional-neural-networks for more information), shares parameters between neurons, and activates these neurons whenever a pattern appears in the image. Using this information, the model is able to effectively train on an image dataset and classify the image as a certain class vector. However, the deconvolutional neural network works in the opposite direction, and the main purpose of using such a network is to generate somewhat original content, such as a picture of a chair (depicted in the image) or possibly 3 hours jazz music. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effectively perform a similar experiment as Cary, I'll need to follow his procedure, changing only the training dataset. Cary used MIDI files as his training dataset, and he plotted the notes on a two-dimensional graph. This created an image that could easily be interpreted by HyperGAN. Y-values represent pitch in semitone intervals, and x-values represent time in 20th of a second intervals (depicted below). \n",
    "<img src=\"notes.png\"/>\n",
    "The music images were scaled to 64x96 pixels, so the training data wasn't too large. However, my computer is significantly slower than Cary's, so I won't be entirely sure until I train my model. Cary also made another modification to the images in order to represent more information per image. Each color channel on the image was made to represent a single pixel, and certain channels were turned on or off to represent different colors and ultimately when certain notes were to be played. This allows the amount of information to be tripled, as 27 bits of information can be represented by only nine pixels (depicted below). \n",
    "<img src=\"pixel.png\"/>\n",
    "There are various \"pros and cons\" to this color channel representation approach, but I won't list them here. Instead, here's a link to Cary's video which starts immediately when these advantages and disadvantages are explained: [Cary's video](https://youtu.be/nA3YOFUCn4U?t=238). Although the end result wasn't very pleasurable to the human ear (and it took about 4 days to train), I'm not exactly looking for creating elegant music. My expectations for my results are extremely low, and I'm hoping to generate an output the at least somewhat resembles music to some extent. I've decided to use HyperGAN to generate jazz music. If I have time, I'll attempt to use PixelCNN because its generated results were far more musical than HyperGAN. However, I'm not especially picky about the quality of the music produced by my model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that I've only just realized is that Cary does not explain how exactly he converted the MIDI files to a comprehensible image. However, he explains this process in a previous video titled \"[Computer evolves to generate baroque music!](https://www.youtube.com/watch?v=SacogDL_4JU&t=88s)\" Cary first visited a website with several free midi files of bach's music. He then used a midi to csv program to create a single text file storing the collection of compositions. After all of this data gathering and processing, Cary noticed something interesting. In the midi/csv file, there is a bit of text in every line which says \"Note_on\" or \"Note_off\" (depicted below). \n",
    "<img src=\"note.png\"/>\n",
    "Cary changed this using a processing script and represented each key of a piano with a different character on the computer keyboard (depicted below). \n",
    "<img src=\"processing.png\"/>\n",
    "Additionally, to allow for different keys to play at the same time, Cary created spaces between notes which represented the passage of time. \n",
    "<img src=\"time.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you might be wondering, \"I thought we were going to use HyperGAN to train our dataset? Doesn't that require us to render our data in image form?\" \n",
    "That's fair. As of now, I don't think I'll be using HyperGAN to train my data. Instead, Cary uses an LSTM for his baroque music video, so that's the model I'll use as well. \n",
    "To understand how an LSTM works, think about it as a recurrent neural network with a \"long-term memory unit.\" This is used as a means of storage for long-term information. The great thing about LSTMs is that it can remember information without having it affect the outputs of neurons directed towards other layers (information taken from Jen Selby's course notes). According to Jen Selby, each layer of the LSTM is made up of four recurrent layers which represent the long-term memory unit. They also contain three gates which perform the following functions: forget (some long-term data is deleted), input (weights decide what information will be added to the long-term memory unit), and the output (weights decide what will be routed to the output). \n",
    "Although I went through all the trouble of describing how it operates, the neural network I'll be using for this project will be an LSTM, which is useful because it takes text as its input. \n",
    "\n",
    "After training the data, the LSTM outputs its generated midi file in its compressed form. A processing script is used to convert it into the csv/midi form, and a csv-midi converter is used to create the final audible music file.\n",
    "\n",
    "Although I'm interested in compression, I don't think I'll write as long of a script to compress the midi/csv files as Cary did. I'm mostly just interested in converting a midi file to csv and feeding that directly into a LSTM. Although the file size is approximately 50 times greater, I'll probably devise a method to make the file size smaller using python (it'll be in this jupyter notebook). \n",
    "\n",
    "Below will be all of the data processing and LSTM setup and training, and I'll explain what I'm doing along the way with comments. After I generate some results, I'll write a reflection about the experience and how I can improve my methods for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Plan: \n",
    "\n",
    "#1. Midi to CSV\n",
    "#2. CSV \"compression\"\n",
    "#3. LSTM setup\n",
    "#4. LSTM training\n",
    "#5. CSV \"decompression\"\n",
    "#6. CSV to Midi\n",
    "#7. Play with Garageband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I've downloaded approximately 30-40 minutes of music, so \n",
    "#I'll convert all of the files to csv and paste them all \n",
    "#into a single document I can train the LSTM with. To convert\n",
    "#the files to csv, I'll be using py_midicsv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_midicsv as pm\n",
    "#midi to csv\n",
    "cast_your_fate = pm.midi_to_csv(\"midi/cast_your_fate.mid\")\n",
    "linus_and_lucy = pm.midi_to_csv(\"midi/linus_lucy.mid\")\n",
    "medley = pm.midi_to_csv(\"midi/medley_ok.mid\")\n",
    "my_drum = pm.midi_to_csv(\"midi/my_drum.mid\")\n",
    "o_tannenbaum = pm.midi_to_csv(\"midi/o_tannenbaum_dwb.mid\")\n",
    "christmas_coming = pm.midi_to_csv(\"midi/christmas_coming.mid\")\n",
    "christmastime = pm.midi_to_csv(\"midi/christmastime.mid\")\n",
    "skating = pm.midi_to_csv(\"midi/skating.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in dataframe form\n",
    "import pandas as pd\n",
    "#to dataframe\n",
    "df_cast = pd.DataFrame(cast_your_fate)\n",
    "df_linus = pd.DataFrame(linus_and_lucy)\n",
    "df_medley = pd.DataFrame(medley)\n",
    "df_drum = pd.DataFrame(my_drum)\n",
    "df_tannenbaum = pd.DataFrame(o_tannenbaum)\n",
    "df_christmas_coming = pd.DataFrame(christmas_coming)\n",
    "df_christmastime = pd.DataFrame(christmastime)\n",
    "df_skating = pd.DataFrame(skating)\n",
    "\n",
    "#saving as csv file\n",
    "#csv_cast_fate = df_cast.to_csv('cast_your_fate.csv')\n",
    "#csv_linus = df_linus.to_csv('linus_lucy.csv')\n",
    "#csv_medley = df_medley.to_csv('medley.csv')\n",
    "#csv_drum = df_drum.to_csv('my_drum.csv')\n",
    "#csv_tannenbaum = df_tannenbaum.to_csv('tannenbaum.csv')\n",
    "#csv_christmas_coming = df_christmas_coming.to_csv('christmas_coming.csv')\n",
    "#csv_christmastime = df_christmastime.to_csv('christmastime.csv')\n",
    "#csv_skating = df_skating.to_csv('cast_your_fate.csv')\n",
    "\n",
    "#concat df \n",
    "df_jazz = pd.concat([df_cast, df_linus, df_medley, df_drum, \n",
    "        df_tannenbaum, df_christmas_coming, df_christmastime, \n",
    "        df_skating], axis=1, join='inner')\n",
    "#saving as a csv file\n",
    "csv_jazz = df_jazz.to_csv('jazz.csv') #724 kB of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a basic LSTM to generate text using the following link: \n",
    "\n",
    "https://towardsdatascience.com/automatically-generate-hotel-descriptions-with-lstm-afa37002d4fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0, 0, Header, 1, 6, 192\\n</td>\n",
       "      <td>0, 0, Header, 1, 11, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 19, 120\\n</td>\n",
       "      <td>0, 0, Header, 1, 11, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 15, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 10, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 12, 96\\n</td>\n",
       "      <td>0, 0, Header, 1, 10, 96\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "      <td>1, 0, Start_track\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1, 0, Text_t, \"Cast Your Fate To The Wind\"\\n</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Key_signature, -3, \"major\"\\n</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "      <td>1, 0, Sequencer_specific, 03, 00, 00, 41\\n</td>\n",
       "      <td>1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0  \\\n",
       "0                     0, 0, Header, 1, 6, 192\\n   \n",
       "1                           1, 0, Start_track\\n   \n",
       "2  1, 0, Text_t, \"Cast Your Fate To The Wind\"\\n   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 11, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                    0  \\\n",
       "0          0, 0, Header, 1, 19, 120\\n   \n",
       "1                 1, 0, Start_track\\n   \n",
       "2  1, 0, Key_signature, -3, \"major\"\\n   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 11, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 15, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                                   0  \\\n",
       "0                          0, 0, Header, 1, 10, 96\\n   \n",
       "1                                1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...   \n",
       "\n",
       "                                            0  \\\n",
       "0                   0, 0, Header, 1, 12, 96\\n   \n",
       "1                         1, 0, Start_track\\n   \n",
       "2  1, 0, Sequencer_specific, 03, 00, 00, 41\\n   \n",
       "\n",
       "                                                   0  \n",
       "0                          0, 0, Header, 1, 10, 96\\n  \n",
       "1                                1, 0, Start_track\\n  \n",
       "2  1, 0, Sequencer_specific, 0A, 00, 00, 5B, 23, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df_jazz.head(n = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>A list of resources is below:</i> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "Huang, Cary. \"AI evolves to compose 3 hours of jazz!\" <i>https://youtube.com,</i> https://www.youtube.com/watch?v=nA3YOFUCn4U. \n",
    "\n",
    "Huang, Cary. \"Computer evolves to generate baroque music!\" <i>https://youtube.com</i> https://youtube.com/watch?v=SacogDL_4JU&t=2s. \n",
    "\n",
    "https://medium.com/machinelearningadvantage/here-are-the-mind-blowing-things-a-deconvolutional-neural-network-can-do-2fc99e008fe4\n",
    "\n",
    "https://github.com/timwedde/py_midicsv\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "https://github.com/karpathy/char-rnn\n",
    "\n",
    "http://midkar.com/\n",
    "\n",
    "https://towardsdatascience.com/automatically-generate-hotel-descriptions-with-lstm-afa37002d4fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
